- name: Install and Configure Spark Cluster
  hosts: all
  become: yes
  vars:
    hadoop_version: "3.3.5"  # Ensure compatibility
    hadoop_home: "/usr/local/hadoop"
    java_home: "/usr/lib/jvm/java-11-openjdk-amd64"
    hadoop_user: "root"

  tasks:
    - name: Ensure /opt/hadoop directory exists
      file:
        path: /opt/hadoop
        state: directory
        mode: '0755'
      become: true  # 提升权限以确保目录创建和文件写入

    - name: Download Hadoop from S3
      aws_s3:
        bucket: "zzbuckent"  # 替换为你的S3 bucket名称
        object: "hadoop-{{ hadoop_version }}.tar.gz"  # S3中的对象路径
        region: "cn-northwest-1"  # 替换为你的 S3 桶所在区域
        dest: "/opt/hadoop/hadoop-{{ hadoop_version }}.tar.gz"
        mode: get  # 下载模式

    - name: Extract Hadoop
      unarchive:
        src: "/opt/hadoop/hadoop-{{ hadoop_version }}.tar.gz"
        dest: "/opt/hadoop/"
        remote_src: yes
      become: true  # 确保拥有足够的权限进行解压

    - name: Ensure symlink /usr/local/hadoop points to the correct Hadoop version
      file:
        src: "/opt/hadoop/hadoop-{{ hadoop_version }}"
        dest: "{{ hadoop_home }}"
        state: link
        force: true  # 强制覆盖已存在的文件或链接
      become: true

    ## 2.设置环境变量 hadoop_home
    - name: Set environment variables for Hadoop
      copy:
        dest: /etc/profile.d/hadoop.sh
        content: |
          export HADOOP_HOME={{ hadoop_home }}
          export PATH=$PATH:$HADOOP_HOME/bin
          export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
          export HADOOP_COMMON_HOME=$HADOOP_HOME
          export HADOOP_HDFS_HOME=$HADOOP_HOME
          export HADOOP_YARN_HOME=$HADOOP_HOME
          export HADOOP_MAPRED_HOME=$HADOOP_HOME
          export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop/
          export HADOOP_COMMON_LIB_NATIVE_DIR=${HADOOP_HOME}/lib/native
          export HADOOP_OPTS="-Djava.library.path=${HADOOP_HOME}/lib/native"
          export LD_LIBRARY_PATH=$HADOOP_COMMON_LIB_NATIVE_DIR:$LD_LIBRARY_PATH

          export HDFS_NAMENODE_USER=ubuntu
          export HDFS_DATANODE_USER=ubuntu
          export HDFS_SECONDARYNAMENODE_USER=ubuntu
          export YARN_RESOURCEMANAGER_USER=ubuntu
          export YARN_NODEMANAGER_USER=ubuntu
        mode: '0644'

    - name: Ensure target directory exists
      file:
        path: /opt/hadoop/hadoop-{{ hadoop_version }}/etc/hadoop/
        state: directory
        mode: '0755'
    - name: Copy config files to all nodes
      copy:
        src: ./{{ item }}
        dest: /opt/hadoop/hadoop-{{ hadoop_version }}/etc/hadoop/{{ item }}
        owner: "{{ ansible_user }}"
        mode: '0644'
      loop:
        - capacity-scheduler.xml
        - core-site.xml
        - hadoop-env.sh
        - hdfs-site.xml
        - mapred-site.xml
        - workers
        - yarn-site.xml
    - name: Format HDFS on master node
      command: sudo -i hadoop namenode -format
      become: true   # 提升为 root 用户执行（如果需要）
      when: "'master' in group_names"
    - name: start hadoop service
      command: sudo -i start-all.sh
      become: true   # 提升为 root 用户执行（如果需要）
      when: "'master' in group_names"
---
- name: Install and Configure Hive Cluster
  hosts: all
  become: yes
  vars:
    hive_version: "4.0.0"  # Adjust Hive version as needed
    hadoop_home: "/usr/local/hadoop"
    hive_home: "/usr/local/hive"
    java_home: "/usr/lib/jvm/java-11-openjdk-amd64"
    mysql_host: "{{ groups['master'][0] }}"  # MySQL 主节点地址
    mysql_database: "hive"
    mysql_user: "hiveuser"
    mysql_password: "your_password"
    mysql_db_type: "mysql"
  handlers:
    - name: Reload systemd
      systemd:
        daemon_reload: yes
  tasks:
    - name: Create Hive directories
      file:
        path: "/opt/hive-{{ hive_version }}"
        state: directory
        owner: ubuntu
        mode: '0755'

    - name: Download Hive from S3
      aws_s3:
        bucket: "zzbuckent"  # 替换为你的 S3 bucket 名称
        object: "hive-{{ hive_version }}.tar.gz"  # S3 中的 Hive 包
        region: "cn-northwest-1"  # 替换为你的 S3 区域
        dest: "/opt/hive-{{ hive_version }}.tar.gz"  # 下载路径
        mode: get

    - name: Extract Hive
      unarchive:
        src: "/opt/hive-{{ hive_version }}.tar.gz"
        dest: "/opt"
        remote_src: yes

    - name: Symlink Hive to /usr/local/hive
      file:
        src: "/opt/hive-{{ hive_version }}"
        dest: "/usr/local/hive"
        state: link

    - name: Set environment variables for Hive
      copy:
        dest: /etc/profile.d/hive.sh
        content: |
          export HIVE_HOME=/usr/local/hive
          export PATH=$PATH:$HIVE_HOME/bin
        mode: '0644'

    - name: Copy config files to all nodes
      copy:
        src: ./{{ item }}
        dest: /usr/local/hive/conf/{{ item }}
        owner: root
        mode: '0644'
      loop:
        - hive-env.sh
        - hive-site.xml

    - name: Initialize Hive schema
      shell: "sudo -i {{ hive_home }}/bin/schematool -initSchema -dbType {{ mysql_db_type }}"
      when: hive_version_output.rc == 0
      environment:
        HIVE_HOME: "{{ hive_home }}"
        PATH: "{{ hive_home }}/bin:{{ ansible_env.PATH }}"
      when: "'master' in group_names"

    - name: Create directories in HDFS for Hive
      shell: |
        export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
        export HADOOP_HOME=/usr/local/hadoop
        export HIVE_HOME=/usr/local/hive
        sudo -i hdfs dfs -mkdir -p /user/hive
        sudo -i hdfs dfs -mkdir -p /user/metastore
        sudo -i hdfs dfs -mkdir -p /user/metastore/warehouse
        sudo -i hdfs dfs -chmod -R 777 /tmp
        sudo -i hdfs dfs -chmod -R 777 /user/hive
        sudo -i hdfs dfs -chmod -R 777 /user/metastore
        sudo -i hdfs dfs -mkdir /user/spark
        sudo -i hdfs dfs -mkdir /user/spark/logs
        sudo -i hdfs dfs -chmod -R 777 /user/spark
      when: "'master' in group_names"

    - name: Create systemd service file for Hive Metastore
      copy:
        dest: /etc/systemd/system/hive-metastore.service
        content: |
          [Unit]
          Description=Hive Metastore Service
          After=network.target

          [Service]
          User=root
          ExecStart=/usr/local/hive/bin/hive --service metastore
          Restart=on-failure

          [Install]
          WantedBy=multi-user.target
      notify:
        - Reload systemd

    - name: Create systemd service file for HiveServer2
      copy:
        dest: /etc/systemd/system/hive-server2.service
        content: |
          [Unit]
          Description=HiveServer2 Service
          After=network.target

          [Service]
          User=root
          ExecStart=/usr/local/hive/bin/hive --service hiveserver2
          Restart=on-failure

          [Install]
          WantedBy=multi-user.target
      notify:
        - Reload systemd

    - name: Ensure Hive Metastore is started and enabled at boot
      systemd:
        name: hive-metastore
        enabled: yes
        state: started

    - name: Ensure HiveServer2 is started and enabled at boot
      systemd:
        name: hive-server2
        enabled: yes
        state: started

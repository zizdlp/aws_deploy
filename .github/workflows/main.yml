name: Deploy and Test Spark Cluster on AWS

on:
  push:
    branches:
      - feature/table  # 当推送到主分支时触发

jobs:
  deploy-and-test:
    runs-on: ubuntu-latest

    steps:
    # Step 1: 检出代码
    - name: Checkout code
      uses: actions/checkout@v3

    # Step 2: 配置 AWS 凭据
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v2
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: cn-northwest-1  # 根据你的实际需求修改

    # Step 3: 添加 SSH 密钥
    - name: Ensure .ssh directory exists
      run: |
        mkdir -p ~/.ssh
        echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/local_test.pem
        chmod 400 ~/.ssh/local_test.pem              
        ls -la ~/.ssh/  # 查看文件列表和权限
    # Step 4: 使用 deps.yml 安装依赖
    - name: Set up Python and Install Boto3
      uses: ./.github/actions/deps
  
    # Step 5: 调用复合操作来创建 EC2 实例
    - name: Deploy EC2 Instances
      uses: ./.github/actions/aws_create

    # Step 6: 从 inventory.ini 文件提取 DNS
    - name: Do Extract all nodes from inventory.ini
      id: extract_nodes
      run: |
        # 提取 master 节点的 DNS
        MASTER_DNS=$(grep -A 1 "\[master\]" ./ansible/inventory.ini | tail -n 1 | awk '{print $1}')

        # 提取 worker 节点的 DNS
        WORKER_DNS=$(awk '/^\[worker\]/ {flag=1; next} /^\[/{flag=0} flag' ./ansible/inventory.ini | awk '{print $1}')

        echo "Master DNS: $MASTER_DNS"
        echo "Worker DNS: $WORKER_DNS"
        
        # 设置 GitHub Actions 输出变量
        echo "::set-output name=master_dns::$MASTER_DNS"
        echo "::set-output name=worker_dns::$WORKER_DNS"

        # 将所有节点的 DNS 添加到 known_hosts
        ssh-keyscan -H $MASTER_DNS >> ~/.ssh/known_hosts
        for WORKER in $WORKER_DNS; do
          ssh-keyscan -H $WORKER >> ~/.ssh/known_hosts
        done

     # Step 7: 测试dns 联通
    - name: Test SSH connection to master
      run: |
        ssh -i ~/.ssh/local_test.pem ubuntu@${{ steps.extract_nodes.outputs.master_dns }} 'hostname'

    - name: Test SSH connection to workers
      run: |
        for WORKER in ${{ steps.extract_nodes.outputs.worker_dns }}; do
          ssh -i ~/.ssh/local_test.pem ubuntu@$WORKER 'hostname'
        done

    # Step 8: 读取 inventory.ini 文件内容
    - name: Read inventory.ini content
      id: read_inventory
      run: |
        cat ./ansible/inventory.ini
        echo "INVENTORY_CONTENT<<EOF" >> $GITHUB_ENV
        cat ./ansible/inventory.ini >> $GITHUB_ENV
        echo "EOF" >> $GITHUB_ENV

    # Step 9: 读取 known_hosts 文件内容并设置为环境变量
    - name: Read known_hosts content
      id: read_known_hosts
      run: |
        cat ~/.ssh/known_hosts
        echo "KNOWN_HOSTS_CONTENT<<EOF" >> $GITHUB_ENV
        cat ~/.ssh/known_hosts >> $GITHUB_ENV
        echo "EOF" >> $GITHUB_ENV
  
    # Step 10: 使用 Ansible 安装依赖
    - name: Install java & deps to nodes
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/install_deps.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}
    # Step 15: 使用 Ansible 部署 sbt
    - name: install sbt
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/install_sbt.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}
    # Step 12: 使用 Ansible 部署 scala
    - name: install scala
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/install_scala.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}
    # Step 10: 使用 Ansible 部署 Spark 集群
    - name: install spark
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/install_spark.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

    # Step 11: 使用 Ansible 部署 hadoop 集群
    - name: install hadoop
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/install_hadoop.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

    # Step 11: 使用 Ansible 部署 mysql
    - name: install mysql
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/install_mysql.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}
    # Step 11: 使用 Ansible 部署 hive
    - name: install hive
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/install_hive.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}
    # Step 15: 使用 Ansible 部署 tpcds
    - name: install tpcds
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/install_tpcds.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}
    # # Step 15: 使用 Ansible 部署 sbt
    # - name: install sbt
    #   uses: dawidd6/action-ansible-playbook@v2
    #   with:
    #     playbook: ./ansible/install_sbt.yml
    #     inventory: ${{ env.INVENTORY_CONTENT }}
    #     key: ${{ secrets.SSH_PRIVATE_KEY }}
    #     known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

    # # Step 13: 运行 Spark 测试
    # - name: Extract Master DNS
    #   id: extract_master_dns
    #   run: |
    #     MASTER_DNS=$(grep '^[master]' ./ansible/inventory.ini | head -n 1)
    #     echo "MASTER_DNS=$MASTER_DNS" >> $GITHUB_ENV

    # # Step 14: Copy 测试文件到master
    # - name: Create directory on master
    #   run: |
    #     MASTER_DNS=${{ env.MASTER_DNS }}
    #     ssh -i ~/.ssh/local_test.pem ubuntu@$MASTER_DNS 'mkdir -p /home/ubuntu/tests'

    # - name: Copy Test cases
    #   run: |
    #     MASTER_DNS=${{ env.MASTER_DNS }}
    #     scp -i ~/.ssh/local_test.pem ./tests/case0.py ubuntu@${MASTER_DNS}:/home/ubuntu/tests/case0.py

    # # Step 15: 测试
    # - name: Run Spark Test
    #   run: |
    #     MASTER_DNS=${{ env.MASTER_DNS }}
    #     ssh -i ~/.ssh/local_test.pem ubuntu@$MASTER_DNS \
    #     "export SPARK_HOME=/usr/local/spark; export PATH=\$PATH:\$SPARK_HOME/bin; spark-submit --master spark://$MASTER_DNS:7077 ~/tests/case0.py"
    # Step 15: 测试
    - name: Run Spark Test
      run: |
        MASTER_DNS=${{ env.MASTER_DNS }}
        ssh -i ~/.ssh/local_test.pem ubuntu@$MASTER_DNS \
        "sudo -i /opt/tpcds-benchmark/run-spark.sh true true true true > result.log 2>&1"
    # # # Step 8: 清理 EC2 实例
    # - name: Terminate EC2 instances
    #   if: always()
    #   run: |
    #     python ./scripts/terminate_ec2_instances.py  # 调用终止实例的 Python 脚本
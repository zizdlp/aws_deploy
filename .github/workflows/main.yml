name: Deploy and Test Spark Cluster on AWS

on:
  push:
    branches:
      - main
jobs:
  deploy-and-test:
    runs-on: ubuntu-latest

    steps:
    # Step 1: 检出代码
    - name: Checkout code
      uses: actions/checkout@v3

    # Step 2: 配置 AWS 凭据
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v2
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: cn-northwest-1

    # Step 3: 添加 SSH 密钥
    - name: Ensure .ssh directory exists
      run: |
        mkdir -p ~/.ssh
        echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/local_test.pem
        chmod 400 ~/.ssh/local_test.pem              
        ls -la ~/.ssh/  # 查看文件列表和权限

    # Step 4: 使用 deps.yml 安装依赖
    - name: Set up Python and Install Boto3
      uses: ./.github/actions/deps
  
    # Step 5: 调用复合操作来创建 EC2 实例
    - name: Deploy EC2 Instances
      uses: ./.github/actions/aws_create

   # Step 4: 从 inventory.ini 文件提取 DNS
    - name: Extract all nodes from inventory.ini
      id: extract_nodes
      run: |
        # 提取节点的 DNS
        MASTER_DNS=$(grep -A 1 "\[master\]" ./ansible/inventory.ini | tail -n 1 | awk '{print $1}')
        WORKER_DNS=$(awk '/^\[worker\]/ {flag=1; next} /^\[/{flag=0} flag' ./ansible/inventory.ini | awk '{print $1}')

        # 设置 GitHub Actions 输出变量
        echo "master_dns=${MASTER_DNS}" >> $GITHUB_OUTPUT
        echo "worker_dns=${WORKER_DNS}" >> $GITHUB_OUTPUT

        # 读取 inventory.ini 文件内容并设置为输出变量
        INVENTORY_CONTENT=$(cat ./ansible/inventory.ini)
        echo "inventory_content=${INVENTORY_CONTENT}" >> $GITHUB_OUTPUT

        # 将所有节点的 DNS 添加到 known_hosts
        ssh-keyscan -H $MASTER_DNS >> ~/.ssh/known_hosts
        for WORKER in $WORKER_DNS; do
          ssh-keyscan -H $WORKER >> ~/.ssh/known_hosts
        done

        # 读取 known_hosts 文件内容并设置为输出变量
        KNOWN_HOSTS_CONTENT=$(cat ~/.ssh/known_hosts)
        echo "known_hosts_content=${KNOWN_HOSTS_CONTENT}" >> $GITHUB_OUTPUT
  
    # Step 5: 使用 Ansible 部署 Spark 集群
    - name: Run playbook
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/install_spark.yml
        inventory: ${{ steps.extract_nodes.outputs.inventory_content }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ steps.extract_nodes.outputs.known_hosts_content }}

    # Step 6: spark test
    - name: Spark Test
      uses: ./.github/actions/spark_test



    # Step 7: 调用复合操作来销毁 EC2 实例
    - name: Terminate EC2 instances
      if: always()
      run: |
        python ./scripts/terminate_ec2_instances.py
      shell: bash  # 添加 shell 指定属性
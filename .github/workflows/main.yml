name: Deploy and Test Spark Cluster on AWS

on:
  push:
    branches:
      - feature/auto2

jobs:
  deploy-and-test:
    runs-on: ubuntu-latest

    steps:
    # Step 1: 检出代码
    - name: Checkout code
      uses: actions/checkout@v3

    # Step 2: 创建aws集群
    - name: Deploy EC2 Instances
      uses: ./.github/actions/aws_create
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: cn-northwest-1  # 根据你的实际需求修改
    # Step 3: 配置ssh & 克隆诸葛弩仓库
    - name: Clone and Compress Repo
      uses: ./.github/actions/clone_and_compress
      with:
        ssh_private_key: ${{ secrets.SSH_PRIVATE_KEY }}
        deploy_key: ${{ secrets.DEPLOY_KEY }}

    # Step 4: 使用 deps.yml 安装依赖
    - name: Set up Python and Install Boto3
      uses: ./.github/actions/deps

    # Step 6: 从 inventory.ini 文件提取 DNS
    - name: Do Extract all nodes from inventory.ini
      id: extract_nodes
      run: |
        # 提取 master 节点的 DNS
        MASTER_DNS=$(grep -A 1 "\[master\]" ./ansible/inventory.ini | tail -n 1 | awk '{print $1}')
    
        # 提取 worker 节点的 DNS
        WORKER_DNS=$(awk '/^\[worker\]/ {flag=1; next} /^\[/{flag=0} flag' ./ansible/inventory.ini | awk '{print $1}' | paste -sd "," -)
    
        echo "Master DNS: $MASTER_DNS"
        echo "Worker DNS: $WORKER_DNS"
    
        # 设置 GitHub Actions 输出变量到环境文件
        echo "MASTER_DNS=$MASTER_DNS" >> $GITHUB_ENV
        echo "WORKER_DNS=$WORKER_DNS" >> $GITHUB_ENV
    
        # 将所有节点的 DNS 添加到 known_hosts，增加重试逻辑
        retry_ssh_keyscan() {
          for i in {1..5}; do
            ssh-keyscan -H $1 >> ~/.ssh/known_hosts && break || sleep 5
          done
        }
    
        retry_ssh_keyscan $MASTER_DNS

        IFS=',' read -ra ADDR <<< "$WORKER_DNS"
        for WORKER in "${ADDR[@]}"; do
          retry_ssh_keyscan $WORKER
        done

    # Step 7: 测试dns 联通
    - name: Test SSH connection to master
      run: |
        ssh -i ~/.ssh/local_test.pem ubuntu@$MASTER_DNS 'hostname'

    - name: Test SSH connection to workers
      run: |
        IFS=',' read -ra ADDR <<< "$WORKER_DNS"
        for WORKER in "${ADDR[@]}"; do
          ssh -i ~/.ssh/local_test.pem ubuntu@$WORKER 'hostname'
        done

    # Step 8: 读取 inventory.ini 文件内容
    - name: Read inventory.ini content
      id: read_inventory
      run: |
        cat ./ansible/inventory.ini
        echo "INVENTORY_CONTENT<<EOF" >> $GITHUB_ENV
        cat ./ansible/inventory.ini >> $GITHUB_ENV
        echo "EOF" >> $GITHUB_ENV

    # Step 9: 读取 known_hosts 文件内容并设置为环境变量
    - name: Read known_hosts content
      id: read_known_hosts
      run: |
        cat ~/.ssh/known_hosts
        echo "KNOWN_HOSTS_CONTENT<<EOF" >> $GITHUB_ENV
        cat ~/.ssh/known_hosts >> $GITHUB_ENV
        echo "EOF" >> $GITHUB_ENV

    # Step 10: 使用 Ansible 安装依赖
    - name: Install java & deps to nodes
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/install_deps.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}
    #Step 11: 配置nvme
    - name: use nvme
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/use_nvme.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}


    # Step 12: 使用 Ansible 部署 sbt
    - name: install sbt
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/install_sbt.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

    # Step 13: 使用 Ansible 部署 scala
    - name: install scala
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/install_scala.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}
    # Step 14: 使用 Ansible 部署 Spark 集群
    - name: install spark
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/install_spark.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

    # Step 15: 使用 Ansible 部署 hadoop 集群
    - name: install hadoop
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/install_hadoop.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

    # Step 16: 使用 Ansible 部署 mysql
    - name: install mysql
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/install_mysql.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

    # Step 17: 使用 Ansible 部署 hive
    - name: install hive
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/install_hive.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

    # Step 18: 使用 Ansible 部署 tpcds
    - name: install tpcds
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/install_tpcds.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}
    - name: install chukonu
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/install_chukonu.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

    - name: Run spark Test
      run: |
        timeout 4h  ssh -o ServerAliveInterval=60 -o ServerAliveCountMax=60 -o StrictHostKeyChecking=no -i ~/.ssh/local_test.pem ubuntu@$MASTER_DNS \
        "cd /opt/tpcds-benchmark && sudo ./run-spark.sh true true false false"
    - name: check spark test result
      run: |
        ssh -o StrictHostKeyChecking=no -i ~/.ssh/local_test.pem ubuntu@$MASTER_DNS \
        "cat /opt/tmp_spark/PowerRuntimes.csv"
    - name: Run chuknou first Test
      run: |
        timeout 4h  ssh -o ServerAliveInterval=60 -o ServerAliveCountMax=60 -o StrictHostKeyChecking=no -i ~/.ssh/local_test.pem ubuntu@$MASTER_DNS \
        "cd /opt/tpcds-benchmark && sudo ./run-chukonu.sh true true false false"
    - name: check chuknou test first result
      run: |
        ssh -o StrictHostKeyChecking=no -i ~/.ssh/local_test.pem ubuntu@$MASTER_DNS \
        "cat /opt/tmp_chuknou/PowerRuntimes.csv"
    - name: Run chuknou second Test
      run: |
        timeout 4h ssh -o ServerAliveInterval=60 -o ServerAliveCountMax=60 -o StrictHostKeyChecking=no -i ~/.ssh/local_test.pem ubuntu@$MASTER_DNS \
        "cd /opt/tpcds-benchmark && sudo ./run-chukonu.sh true true false false"
    - name: check chuknou test second result
      run: |
        ssh -o StrictHostKeyChecking=no -i ~/.ssh/local_test.pem ubuntu@$MASTER_DNS \
        "cat /opt/tmp_chuknou/PowerRuntimes.csv"
    # # # Step 8: 清理 EC2 实例
    - name: Terminate EC2 instances
      if: always()
      run: |
        python ./scripts/terminate_ec2_instances.py  # 调用终止实例的 Python 脚本
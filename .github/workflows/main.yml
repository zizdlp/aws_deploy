name: Deploy and Test Spark Cluster on AWS

on:
  push:
    branches:
      - main

jobs:
  deploy-and-test:
    runs-on: ubuntu-latest

    steps:
    # Step 1: 检出代码
    - name: Checkout code
      uses: actions/checkout@v3

    # Step 2: 配置 AWS 凭据
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v2
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: cn-northwest-1

    # Step 3: 添加 SSH 密钥
    - name: Ensure .ssh directory exists
      run: |
        mkdir -p ~/.ssh
        echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/local_test.pem
        chmod 400 ~/.ssh/local_test.pem              
        ls -la ~/.ssh/  # 查看文件列表和权限

    # Step 3: 使用 deps.yml 安装依赖
    - name: Set up Python and Install Boto3
      uses: ./.github/actions/deps
  
    # Step 4: 调用复合操作来创建 EC2 实例
    - name: Deploy EC2 Instances
      uses: ./.github/actions/aws_create


    # Step 5: 使用 Ansible 部署 Spark 集群
    - name: Run playbook
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/install_spark.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

    # Step 6: spark test
    - name: Spark Test
      uses: ./.github/actions/spark_test



    # Step 7: 调用复合操作来销毁 EC2 实例
    - name: Terminate EC2 Instances
      uses: ./.github/actions/aws_destory

name: TEST_TPCDS

on:
  # Manually trigger the workflow
  workflow_dispatch:
    inputs:
      branch:
        description: "The branch to trigger the workflow on"
        required: false
        default: "main"
  pull_request:
    branches:
      - main
  push:
    branches:
      - main
env:  # Define environment variables here
  NUM_INSTANCES: 4  # Set the number of instances here
  INSTANCE_TYPE: i4i.4xlarge  # Set the instance type here
  SCALE_FACTOR: 100
  SPARK_NUM_EXECUTORS: 3
  SPARK_EXECUTOR_CORES: 12
  SPARK_EXECUTOR_MEMORY: 90g
  SPARK_OFFSIZE: 0g
  SPARK_OVERHEAD: 6g
  CHUKONU_NUM_EXECUTORS: 3
  CHUKONU_EXECUTOR_CORES: 12
  CHUKONU_EXECUTOR_MEMORY: 47g
  CHUKONU_OFFSIZE: 1g
  CHUKONU_OVERHEAD: 48g
  NODEMANAGER_MEMORY_MB: 102400
  MAXIMUM_ALLOCATION_MB: 307200
  MINIMUM_ALLOCATION_MB: 2048
  MAXIMUM_ALLOCATION_VCORES: 45
  # SPARK_RESULT: '0'
  # CHUKONU_RESULT: '0'
  SPARK_RESULT: "https://github.com/zizdlp/aws_deploy/actions/runs/11494315795/artifacts/2101938630"
  CHUKONU_RESULT: "https://github.com/zizdlp/aws_deploy/actions/runs/11494315795/artifacts/2101938627"
jobs:
  run-spark-test:
    runs-on: ubuntu-latest
    timeout-minutes: 1200
    strategy:
      matrix:
        runner: [spark,chukonu]
    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Install deps for worker
      uses: ./.github/actions/install_deps

    - name: Deploy EC2 Instances
      if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0')
      uses: ./.github/actions/aws_create
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        num-instances: ${{ env.NUM_INSTANCES }}
        instance-type: ${{ env.INSTANCE_TYPE }}
        runner: ${{ matrix.runner }}
    - name: Clone and Compress Repo
      if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0')
      uses: ./.github/actions/clone_and_compress
      with:
        ssh_private_key: ${{ secrets.SSH_PRIVATE_KEY }}
        deploy_key: ${{ secrets.DEPLOY_KEY }}

    - name: Test SSH connection to master
      if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0')
      run: |
        ssh -i ~/.ssh/local_test.pem ubuntu@node0 'hostname'

    - name: Print Environment Variables
      if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0')
      run: |
        echo "NUM_INSTANCES: $NUM_INSTANCES"
        echo "INSTANCE_TYPE: $INSTANCE_TYPE"

    - name: Update vars with env
      if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0')
      run: |
        python3 ./.github/scripts/update_var.py 

    - name: Read inventory.ini content
      if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0')
      id: read_inventory
      run: |
        cat ./ansible/inventory.ini
        echo "INVENTORY_CONTENT<<EOF" >> $GITHUB_ENV
        cat ./ansible/inventory.ini >> $GITHUB_ENV
        echo "EOF" >> $GITHUB_ENV

    - name: Read known_hosts content
      if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0')
      id: read_known_hosts
      run: |
        cat ~/.ssh/known_hosts
        echo "KNOWN_HOSTS_CONTENT<<EOF" >> $GITHUB_ENV
        cat ~/.ssh/known_hosts >> $GITHUB_ENV
        echo "EOF" >> $GITHUB_ENV

    - name: set ssh & nfs
      if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0')
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/install_deps.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

    - name: use nvme
      if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0')
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/use_nvme.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

    - name: install sbt
      if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0')
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/install_sbt.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

    - name: install scala
      if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0')
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/install_scala.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

    - name: install spark
      if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0')
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/install_spark.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

    - name: install hadoop
      if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0')
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/install_hadoop.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

    - name: install mysql 
      if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0')
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/install_mysql.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

    - name: install hive
      if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0')
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/install_hive.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

    - name: install tpcds
      if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0')
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/install_tpcds.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}
    - name: install chukonu
      if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0')
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/install_chukonu.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

    - name: Run spark Test
      if: matrix.runner == 'spark' && env.SPARK_RESULT == '0'
      run: |
        timeout 4h ssh -o ServerAliveInterval=60 -o ServerAliveCountMax=60 -o StrictHostKeyChecking=no -i ~/.ssh/local_test.pem ubuntu@node0 \
        "cd /opt/tpcds-benchmark && sudo ./run-spark.sh true true false false > ~/spark_test_log.txt 2>&1"

    - name: download spark test log
      if: matrix.runner == 'spark' && env.SPARK_RESULT == '0'
      run: |
        scp -o StrictHostKeyChecking=no -i ~/.ssh/local_test.pem ubuntu@node0:~/spark_test_log.txt spark_test_log.txt
        scp -o StrictHostKeyChecking=no -i ~/.ssh/local_test.pem ubuntu@node0:/opt/tmp_spark/PowerRuntimes.csv spark_result.csv
    - name: upload spark test log
      if: matrix.runner == 'spark' && env.SPARK_RESULT == '0'
      uses: actions/upload-artifact@v3
      with:
        name: spark_test_log.txt
        path: spark_test_log.txt
    - name: upload spark result
      if: matrix.runner == 'spark' && env.SPARK_RESULT == '0'
      uses: actions/upload-artifact@v3
      with:
        name: spark_result.csv
        path: spark_result.csv

    - name: Run chuknou first Test
      if: matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0'
      run: |
        timeout 4h ssh -o ServerAliveInterval=60 -o ServerAliveCountMax=60 -o StrictHostKeyChecking=no -i ~/.ssh/local_test.pem ubuntu@node0 \
        "cd /opt/tpcds-benchmark && sudo ./run-chukonu.sh true true false false > ~/chuknou_test_log.txt 2>&1"
    - name: download chuknou test log
      if: matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0'
      run: |
        scp -o StrictHostKeyChecking=no -i ~/.ssh/local_test.pem ubuntu@node0:~/chuknou_test_log.txt chuknou_test_log.txt

    - name: upload chuknou test log
      if: matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0'
      uses: actions/upload-artifact@v3
      with:
        name: chuknou_test_log.txt
        path: chuknou_test_log.txt

    - name: Run chuknou second Test
      if: matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0'
      run: |
        timeout 4h ssh -o ServerAliveInterval=60 -o ServerAliveCountMax=60 -o StrictHostKeyChecking=no -i ~/.ssh/local_test.pem ubuntu@node0 \
        "cd /opt/tpcds-benchmark && sudo ./run-chukonu.sh true true false false"
    # 使用 scp 将文件从 node0 拷贝到宿主
    - name: download tpcds result from node0
      if: matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0'
      run: |
        scp -o StrictHostKeyChecking=no -i ~/.ssh/local_test.pem ubuntu@node0:/opt/tmp_chukonu/PowerRuntimes.csv chukonu_result.csv
    - name: upload chukonu result
      if: matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0'
      uses: actions/upload-artifact@v3
      with:
        name: chukonu_result.csv
        path: chukonu_result.csv
    - name: Terminate EC2 instances
      if: always()
      run: |
        python ./.github/scripts/terminate_ec2_instances.py --runner ${{ matrix.runner }} \
         --spark ${{ env.SPARK_RESULT }} \
         --chukonu ${{ env.CHUKONU_RESULT }}
  aggregate-results:
    runs-on: ubuntu-latest
    needs: run-spark-test
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      # Step 1: 配置python
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'
      # Step 2: 安装 pip依赖
      - name: Install Boto3
        run: |
          python -m pip install --upgrade pip
          pip install pandas

      - name: Download spark result using actions/download-artifact if SPARK_RESULT is "0"
        if: env.SPARK_RESULT == '0'
        uses: actions/download-artifact@v3
        with:
          name: spark_result.csv

      - name: Download spark result using curl if SPARK_RESULT is not "0"
        if: env.SPARK_RESULT != '0'
        run: |
          curl -L -H "Authorization: token ${{ secrets.ACT_TOKEN }}" \
            -o spark_result.zip \
            ${{env.SPARK_RESULT}}/zip
          unzip spark_result.zip

      - name: Download CHUKONU_RESULT using actions/download-artifact if CHUKONU_RESULT is "0"
        if: env.CHUKONU_RESULT == '0'
        uses: actions/download-artifact@v3
        with:
          name: chukonu_result.csv

      - name: Download CHUKONU_RESULT result using curl if CHUKONU_RESULT is not "0"
        if: env.CHUKONU_RESULT != '0'
        run: |
          curl -L -H "Authorization: token ${{ secrets.ACT_TOKEN }}" \
            -o chukonu_result.zip \
            ${{env.CHUKONU_RESULT}}
          unzip chukonu_result.zip

      - name: Compare Test Results
        run: python3 ./.github/scripts/compare.py --spark ./spark_result.csv

      - name: Upload Comparison Results
        uses: actions/upload-artifact@v3
        with:
          name: tpcds_result.csv
          path: tpcds_result.csv
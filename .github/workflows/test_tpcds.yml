name: TEST_TPCDS

on:
  # Manually trigger the workflow
  workflow_dispatch:
    inputs:
      branch:
        description: "The branch to trigger the workflow on"
        required: false
        default: "main"
      scale_factor:
        description: "Scale factor"
        required: false
        default: "100"
      spark_result:
        description: "Spark result URL"
        required: false
        default: "https://api.github.com/repos/chukonu-team/chukonu/actions/artifacts/2115691737/zip" # default use previous test case: 100g
      chukonu_result:
        description: "Chukonu result URL"
        required: false
        default: "0"
      commit_id:
        description: "Chukonu commit id"
        required: false
        default: ""
  pull_request:
    branches:
      - main
  push:
    branches:
      - main

env:
  NUM_INSTANCES: 4
  INSTANCE_TYPE:  'i4i.4xlarge'
  SCALE_FACTOR: ${{ github.event.inputs.scale_factor || 100 }}
  SPARK_NUM_EXECUTORS: 3
  SPARK_EXECUTOR_CORES: 12
  SPARK_EXECUTOR_MEMORY:  '90g'
  SPARK_OFFSIZE:   '0g' 
  SPARK_OVERHEAD:  '6g' 
  CHUKONU_NUM_EXECUTORS:  3
  CHUKONU_EXECUTOR_CORES:  12
  CHUKONU_EXECUTOR_MEMORY:  '47g'
  CHUKONU_OFFSIZE:  '1g' 
  CHUKONU_OVERHEAD:  '48g'
  NODEMANAGER_MEMORY_MB:  102400
  MAXIMUM_ALLOCATION_MB:  307200
  MINIMUM_ALLOCATION_MB:  2048
  MAXIMUM_ALLOCATION_VCORES:  45
  SPARK_RESULT: ${{ github.event.inputs.spark_result || '0' }} # default use previous test case: 100g
  CHUKONU_RESULT: ${{ github.event.inputs.chukonu_result || '0' }} # default rerun this test
  COMMIT_ID: ${{ github.event.inputs.commit_id || '' }}
  # SPARK_RESULT: '0' # run test
  # CHUKONU_RESULT: '0' # run test
  # SPARK_RESULT: "https://api.github.com/repos/chukonu-team/chukonu/actions/artifacts/2115691737/zip" # 100g,keep only 400days,start from 2024-1029
  # CHUKONU_RESULT: "https://api.github.com/repos/chukonu-team/chukonu/actions/artifacts/2115691734/zip" # 100g,keep only 400days,start from 2024-1029

  # SPARK_RESULT: "https://api.github.com/repos/chukonu-team/chukonu/actions/artifacts/2110549627/zip" # 1000g,keep only 90days,start from 2024-1028
  # CHUKONU_RESULT: "https://api.github.com/repos/chukonu-team/chukonu/actions/artifacts/2110549625/zip" # 1000g,keep only 90days,start from 2024-1028
jobs:
  run-spark-test:
    runs-on: ubuntu-latest
    timeout-minutes: 1200
    strategy:
      matrix:
        runner: [spark,chukonu]
    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    # - name: Install deps for worker
    #   uses: ./.github/actions/install_deps

    # - name: Deploy EC2 Instances
    #   if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0')
    #   uses: ./.github/actions/aws_create
    #   with:
    #     aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
    #     aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
    #     num-instances: ${{ env.NUM_INSTANCES }}
    #     instance-type: ${{ env.INSTANCE_TYPE }}
    #     runner: ${{ matrix.runner }}
    #     run-number: ${{ github.run_number }}
    # - name: Clone and Compress Repo
    #   if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0')
    #   uses: ./.github/actions/clone_and_compress
    #   with:
    #     ssh_private_key: ${{ secrets.SSH_PRIVATE_KEY }}
    #     deploy_key: ${{ secrets.DEPLOY_KEY }}
    #     commit_id: ${{ env.COMMIT_ID }}

    # - name: Test SSH connection to master
    #   if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0')
    #   run: |
    #     ssh -i ~/.ssh/local_test.pem ubuntu@node0 'hostname'

    # - name: Print Environment Variables
    #   if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0')
    #   run: |
    #     echo "NUM_INSTANCES: $NUM_INSTANCES"
    #     echo "INSTANCE_TYPE: $INSTANCE_TYPE"

    # - name: Update vars with env
    #   if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0')
    #   run: |
    #     python3 ./.github/scripts/update_var.py 

    # - name: Read inventory.ini content
    #   if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0')
    #   id: read_inventory
    #   run: |
    #     cat ./ansible/inventory.ini
    #     echo "INVENTORY_CONTENT<<EOF" >> $GITHUB_ENV
    #     cat ./ansible/inventory.ini >> $GITHUB_ENV
    #     echo "EOF" >> $GITHUB_ENV

    # - name: Read known_hosts content
    #   if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0')
    #   id: read_known_hosts
    #   run: |
    #     cat ~/.ssh/known_hosts
    #     echo "KNOWN_HOSTS_CONTENT<<EOF" >> $GITHUB_ENV
    #     cat ~/.ssh/known_hosts >> $GITHUB_ENV
    #     echo "EOF" >> $GITHUB_ENV

    # - name: set ssh & nfs
    #   if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0')
    #   uses: dawidd6/action-ansible-playbook@v2
    #   with:
    #     playbook: ./ansible/install_deps.yml
    #     inventory: ${{ env.INVENTORY_CONTENT }}
    #     key: ${{ secrets.SSH_PRIVATE_KEY }}
    #     known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

    # - name: use nvme
    #   if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0')
    #   uses: dawidd6/action-ansible-playbook@v2
    #   with:
    #     playbook: ./ansible/use_nvme.yml
    #     inventory: ${{ env.INVENTORY_CONTENT }}
    #     key: ${{ secrets.SSH_PRIVATE_KEY }}
    #     known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

    # - name: install sbt
    #   if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0')
    #   uses: dawidd6/action-ansible-playbook@v2
    #   with:
    #     playbook: ./ansible/install_sbt.yml
    #     inventory: ${{ env.INVENTORY_CONTENT }}
    #     key: ${{ secrets.SSH_PRIVATE_KEY }}
    #     known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

    # - name: install scala
    #   if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0')
    #   uses: dawidd6/action-ansible-playbook@v2
    #   with:
    #     playbook: ./ansible/install_scala.yml
    #     inventory: ${{ env.INVENTORY_CONTENT }}
    #     key: ${{ secrets.SSH_PRIVATE_KEY }}
    #     known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

    # - name: install spark
    #   if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0')
    #   uses: dawidd6/action-ansible-playbook@v2
    #   with:
    #     playbook: ./ansible/install_spark.yml
    #     inventory: ${{ env.INVENTORY_CONTENT }}
    #     key: ${{ secrets.SSH_PRIVATE_KEY }}
    #     known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

    # - name: install hadoop
    #   if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0')
    #   uses: dawidd6/action-ansible-playbook@v2
    #   with:
    #     playbook: ./ansible/install_hadoop.yml
    #     inventory: ${{ env.INVENTORY_CONTENT }}
    #     key: ${{ secrets.SSH_PRIVATE_KEY }}
    #     known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

    # - name: install mysql 
    #   if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0')
    #   uses: dawidd6/action-ansible-playbook@v2
    #   with:
    #     playbook: ./ansible/install_mysql.yml
    #     inventory: ${{ env.INVENTORY_CONTENT }}
    #     key: ${{ secrets.SSH_PRIVATE_KEY }}
    #     known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

    # - name: install hive
    #   if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0')
    #   uses: dawidd6/action-ansible-playbook@v2
    #   with:
    #     playbook: ./ansible/install_hive.yml
    #     inventory: ${{ env.INVENTORY_CONTENT }}
    #     key: ${{ secrets.SSH_PRIVATE_KEY }}
    #     known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

    # - name: install tpcds
    #   if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0')
    #   uses: dawidd6/action-ansible-playbook@v2
    #   with:
    #     playbook: ./ansible/install_tpcds.yml
    #     inventory: ${{ env.INVENTORY_CONTENT }}
    #     key: ${{ secrets.SSH_PRIVATE_KEY }}
    #     known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}
    # - name: install chukonu
    #   if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0')
    #   uses: dawidd6/action-ansible-playbook@v2
    #   with:
    #     playbook: ./ansible/install_chukonu.yml
    #     inventory: ${{ env.INVENTORY_CONTENT }}
    #     key: ${{ secrets.SSH_PRIVATE_KEY }}
    #     known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}
    # - name: Run gen data
    #   if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0')
    #   run: |
    #     ssh -o ServerAliveInterval=60 -o ServerAliveCountMax=60 -o StrictHostKeyChecking=no -i ~/.ssh/local_test.pem ubuntu@node0 \
    #     "sudo ./gen_data.sh ${{matrix.runner}} ${{env.SCALE_FACTOR}}"
    # - name: Run create table
    #   if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0')
    #   run: |
    #     ssh -o ServerAliveInterval=60 -o ServerAliveCountMax=60 -o StrictHostKeyChecking=no -i ~/.ssh/local_test.pem ubuntu@node0 \
    #     "sudo ./create_table.sh ${{matrix.runner}}"

    # - name: Run First Test
    #   if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0')
    #   run: |
    #     ssh -o ServerAliveInterval=60 -o ServerAliveCountMax=60 -o StrictHostKeyChecking=no -i ~/.ssh/local_test.pem ubuntu@node0 \
    #     "sudo ./99query.sh ${{matrix.runner}}; python3 process_data.py --runner ${{matrix.runner}}"
    # - name: Run Second Test
    #   if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0')
    #   run: |
    #     ssh -o ServerAliveInterval=60 -o ServerAliveCountMax=60 -o StrictHostKeyChecking=no -i ~/.ssh/local_test.pem ubuntu@node0 \
    #     "sudo ./99query.sh ${{matrix.runner}}; python3 process_data.py --runner ${{matrix.runner}}"
    # - name: download spark test log
    #   if: matrix.runner == 'spark' && env.SPARK_RESULT == '0'
    #   run: |
    #     scp -o StrictHostKeyChecking=no -i ~/.ssh/local_test.pem ubuntu@node0:~/spark_test_log.txt spark_test_log.txt
    #     scp -o StrictHostKeyChecking=no -i ~/.ssh/local_test.pem ubuntu@node0:/opt/tmp_spark/PowerRuntimes.csv spark_result.csv
    # - name: upload spark test log
    #   if: matrix.runner == 'spark' && env.SPARK_RESULT == '0'
    #   uses: actions/upload-artifact@v3
    #   with:
    #     name: spark_test_log.txt
    #     path: spark_test_log.txt
    #     retention-days: 400
    # - name: upload spark result
    #   if: matrix.runner == 'spark' && env.SPARK_RESULT == '0'
    #   uses: actions/upload-artifact@v3
    #   with:
    #     name: spark_result.csv
    #     path: spark_result.csv
    #     retention-days: 400
    # - name: Run chuknou first Test
    #   if: matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0'
    #   run: |
    #     timeout 4h ssh -o ServerAliveInterval=60 -o ServerAliveCountMax=60 -o StrictHostKeyChecking=no -i ~/.ssh/local_test.pem ubuntu@node0 \
    #     "cd /opt/tpcds-benchmark && sudo ./run-chukonu.sh true true false false > ~/chuknou_test_log.txt 2>&1"
    # - name: download chuknou test log
    #   if: matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0'
    #   run: |
    #     scp -o StrictHostKeyChecking=no -i ~/.ssh/local_test.pem ubuntu@node0:~/chuknou_test_log.txt chuknou_test_log.txt

    # - name: upload chuknou test log
    #   if: matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0'
    #   uses: actions/upload-artifact@v3
    #   with:
    #     name: chuknou_test_log.txt
    #     path: chuknou_test_log.txt
    #     retention-days: 400
    # - name: Run chuknou second Test
    #   if: matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0'
    #   run: |
    #     timeout 4h ssh -o ServerAliveInterval=60 -o ServerAliveCountMax=60 -o StrictHostKeyChecking=no -i ~/.ssh/local_test.pem ubuntu@node0 \
    #     "cd /opt/tpcds-benchmark && sudo ./run-chukonu.sh true true false false"
    # # 使用 scp 将文件从 node0 拷贝到宿主
    # - name: download tpcds result from node0
    #   if: matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0'
    #   run: |
    #     scp -o StrictHostKeyChecking=no -i ~/.ssh/local_test.pem ubuntu@node0:/opt/tmp_chukonu/PowerRuntimes.csv chukonu_result.csv
    # - name: upload chukonu result
    #   if: matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0'
    #   uses: actions/upload-artifact@v3
    #   with:
    #     name: chukonu_result.csv
    #     path: chukonu_result.csv
    #     retention-days: 400
    # # Step 1: 压缩 node0 上的 /opt/event_logs
    # - name: Compress event_logs on node0
    #   if: matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0'
    #   run: |
    #     ssh -o StrictHostKeyChecking=no -i ~/.ssh/local_test.pem ubuntu@node0 "sudo tar -czf /opt/event_logs.tar.gz -C /opt event_logs"
    #     scp -o StrictHostKeyChecking=no -i ~/.ssh/local_test.pem ubuntu@node0:/opt/event_logs.tar.gz event_logs.tar.gz
    # - name: Upload event_logs
    #   if: matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0'
    #   uses: actions/upload-artifact@v3
    #   with:
    #     name: event_logs.tar.gz
    #     path: event_logs.tar.gz
    #     retention-days: 400
    # - name: Compress chukonu_cache on node0
    #   if: matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0'
    #   run: |
    #     ssh -o StrictHostKeyChecking=no -i ~/.ssh/local_test.pem ubuntu@node0 "sudo tar -czf /opt/chukonu_cache.tar.gz -C /opt chukonu_cache"
    #     scp -o StrictHostKeyChecking=no -i ~/.ssh/local_test.pem ubuntu@node0:/opt/chukonu_cache.tar.gz chukonu_cache.tar.gz
    # - name: Upload chukonu_cache
    #   if: matrix.runner == 'chukonu' && env.CHUKONU_RESULT == '0'
    #   uses: actions/upload-artifact@v3
    #   with:
    #     name: chukonu_cache.tar.gz
    #     path: chukonu_cache.tar.gz
    #     retention-days: 400

    # - name: Terminate EC2 instances
    #   if: always()
    #   run: |
    #     python ./.github/scripts/terminate_ec2_instances.py --runner ${{ matrix.runner }} \
    #      --spark ${{ env.SPARK_RESULT }} \
    #      --chukonu ${{ env.CHUKONU_RESULT }} \
    #      --run-number ${{ github.run_number }}
  # aggregate-results:
  #   runs-on: ubuntu-latest
  #   needs: run-spark-test
  #   steps:
  #     - name: Checkout code
  #       uses: actions/checkout@v3
  #     # Step 1: 配置python
  #     - name: Set up Python
  #       uses: actions/setup-python@v4
  #       with:
  #         python-version: '3.x'
  #     # Step 2: 安装 pip依赖
  #     - name: Install Boto3
  #       run: |
  #         python -m pip install --upgrade pip
  #         pip install pandas

  #     - name: Download spark result using actions/download-artifact if SPARK_RESULT is "0"
  #       if: env.SPARK_RESULT == '0'
  #       uses: actions/download-artifact@v3
  #       with:
  #         name: spark_result.csv

  #     - name: Download spark result using curl if SPARK_RESULT is not "0"
  #       if: env.SPARK_RESULT != '0'
  #       run: |
  #         curl -L -H "Authorization: token ${{ secrets.ACT_TOKEN }}" \
  #           -o spark_result.zip \
  #           ${{env.SPARK_RESULT}}
  #         ls -lh
  #         unzip spark_result.zip

  #     - name: Download CHUKONU_RESULT using actions/download-artifact if CHUKONU_RESULT is "0"
  #       if: env.CHUKONU_RESULT == '0'
  #       uses: actions/download-artifact@v3
  #       with:
  #         name: chukonu_result.csv

  #     - name: Download CHUKONU_RESULT result using curl if CHUKONU_RESULT is not "0"
  #       if: env.CHUKONU_RESULT != '0'
  #       run: |
  #         curl -L -H "Authorization: token ${{ secrets.ACT_TOKEN }}" \
  #           -o chukonu_result.zip \
  #           ${{env.CHUKONU_RESULT}}
  #         unzip chukonu_result.zip

  #     - name: Compare Test Results
  #       run: python3 ./.github/scripts/compare.py --spark ./spark_result.csv

  #     - name: Upload Comparison Results
  #       uses: actions/upload-artifact@v3
  #       with:
  #         name: tpcds_result.csv
  #         path: tpcds_result.csv
  #         retention-days: 400
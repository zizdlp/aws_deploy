name: TEST_TPCDS

on:
  pull_request:
    branches:
      - main
  push:
    branches:
      - main
env:  # Define environment variables here
  NUM_INSTANCES: 4  # Set the number of instances here
  INSTANCE_TYPE: i4i.4xlarge  # Set the instance type here
  SCALE_FACTOR: 100
  SPARK_NUM_EXECUTORS: 3
  SPARK_EXECUTOR_CORES: 12
  SPARK_EXECUTOR_MEMORY: 90g
  SPARK_OFFSIZE: 0g
  SPARK_OVERHEAD: 6g
  CHUKONU_NUM_EXECUTORS: 3
  CHUKONU_EXECUTOR_CORES: 12
  CHUKONU_EXECUTOR_MEMORY: 47g
  CHUKONU_OFFSIZE: 1g
  CHUKONU_OVERHEAD: 48g
  NODEMANAGER_MEMORY_MB: 102400
  MAXIMUM_ALLOCATION_MB: 307200
  MINIMUM_ALLOCATION_MB: 2048
  MAXIMUM_ALLOCATION_VCORES: 45
  SPARK_RUNNER: 53 #不是0则意味着使用之前测试的spark，这里只测试诸葛弩，所以对应的下面runner: [chukonu]
  ARTIFACT_ID: 2101938630
jobs:
  # run-spark-test:
  #   runs-on: ubuntu-latest
  #   timeout-minutes: 1200
  #   strategy:
  #     matrix:
  #       runner: [spark,chukonu] # spark, chukonu
  #   steps:
  #   - name: Checkout code
  #     uses: actions/checkout@v3

  #   - name: Install deps for worker
  #     uses: ./.github/actions/install_deps

  #   - name: Deploy EC2 Instances
  #     uses: ./.github/actions/aws_create
  #     with:
  #       aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
  #       aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
  #       num-instances: ${{ env.NUM_INSTANCES }}
  #       instance-type: ${{ env.INSTANCE_TYPE }}
  #       runner: ${{ matrix.runner }}
  #   - name: Clone and Compress Repo
  #     uses: ./.github/actions/clone_and_compress
  #     with:
  #       ssh_private_key: ${{ secrets.SSH_PRIVATE_KEY }}
  #       deploy_key: ${{ secrets.DEPLOY_KEY }}

  #   - name: Test SSH connection to master
  #     run: |
  #       ssh -i ~/.ssh/local_test.pem ubuntu@node0 'hostname'

  #   - name: Print Environment Variables
  #     run: |
  #       echo "NUM_INSTANCES: $NUM_INSTANCES"
  #       echo "INSTANCE_TYPE: $INSTANCE_TYPE"

  #   - name: Update vars with env
  #     run: |
  #       python3 ./.github/scripts/update_var.py 

  #   - name: Read inventory.ini content
  #     id: read_inventory
  #     run: |
  #       cat ./ansible/inventory.ini
  #       echo "INVENTORY_CONTENT<<EOF" >> $GITHUB_ENV
  #       cat ./ansible/inventory.ini >> $GITHUB_ENV
  #       echo "EOF" >> $GITHUB_ENV

  #   - name: Read known_hosts content
  #     id: read_known_hosts
  #     run: |
  #       cat ~/.ssh/known_hosts
  #       echo "KNOWN_HOSTS_CONTENT<<EOF" >> $GITHUB_ENV
  #       cat ~/.ssh/known_hosts >> $GITHUB_ENV
  #       echo "EOF" >> $GITHUB_ENV

  #   - name: set ssh & nfs
  #     uses: dawidd6/action-ansible-playbook@v2
  #     with:
  #       playbook: ./ansible/install_deps.yml
  #       inventory: ${{ env.INVENTORY_CONTENT }}
  #       key: ${{ secrets.SSH_PRIVATE_KEY }}
  #       known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

  #   - name: use nvme
  #     uses: dawidd6/action-ansible-playbook@v2
  #     with:
  #       playbook: ./ansible/use_nvme.yml
  #       inventory: ${{ env.INVENTORY_CONTENT }}
  #       key: ${{ secrets.SSH_PRIVATE_KEY }}
  #       known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

  #   - name: install sbt
  #     uses: dawidd6/action-ansible-playbook@v2
  #     with:
  #       playbook: ./ansible/install_sbt.yml
  #       inventory: ${{ env.INVENTORY_CONTENT }}
  #       key: ${{ secrets.SSH_PRIVATE_KEY }}
  #       known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

  #   - name: install scala
  #     uses: dawidd6/action-ansible-playbook@v2
  #     with:
  #       playbook: ./ansible/install_scala.yml
  #       inventory: ${{ env.INVENTORY_CONTENT }}
  #       key: ${{ secrets.SSH_PRIVATE_KEY }}
  #       known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

  #   - name: install spark
  #     uses: dawidd6/action-ansible-playbook@v2
  #     with:
  #       playbook: ./ansible/install_spark.yml
  #       inventory: ${{ env.INVENTORY_CONTENT }}
  #       key: ${{ secrets.SSH_PRIVATE_KEY }}
  #       known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

  #   - name: install hadoop
  #     uses: dawidd6/action-ansible-playbook@v2
  #     with:
  #       playbook: ./ansible/install_hadoop.yml
  #       inventory: ${{ env.INVENTORY_CONTENT }}
  #       key: ${{ secrets.SSH_PRIVATE_KEY }}
  #       known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

  #   - name: install mysql
  #     uses: dawidd6/action-ansible-playbook@v2
  #     with:
  #       playbook: ./ansible/install_mysql.yml
  #       inventory: ${{ env.INVENTORY_CONTENT }}
  #       key: ${{ secrets.SSH_PRIVATE_KEY }}
  #       known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

  #   - name: install hive
  #     uses: dawidd6/action-ansible-playbook@v2
  #     with:
  #       playbook: ./ansible/install_hive.yml
  #       inventory: ${{ env.INVENTORY_CONTENT }}
  #       key: ${{ secrets.SSH_PRIVATE_KEY }}
  #       known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

  #   - name: install tpcds
  #     uses: dawidd6/action-ansible-playbook@v2
  #     with:
  #       playbook: ./ansible/install_tpcds.yml
  #       inventory: ${{ env.INVENTORY_CONTENT }}
  #       key: ${{ secrets.SSH_PRIVATE_KEY }}
  #       known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}
  #   - name: install chukonu
  #     uses: dawidd6/action-ansible-playbook@v2
  #     with:
  #       playbook: ./ansible/install_chukonu.yml
  #       inventory: ${{ env.INVENTORY_CONTENT }}
  #       key: ${{ secrets.SSH_PRIVATE_KEY }}
  #       known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

  #   - name: Run spark Test
  #     if: matrix.runner == 'spark' 
  #     run: |
  #       timeout 4h ssh -o ServerAliveInterval=60 -o ServerAliveCountMax=60 -o StrictHostKeyChecking=no -i ~/.ssh/local_test.pem ubuntu@node0 \
  #       "cd /opt/tpcds-benchmark && sudo ./run-spark.sh true true false false > ~/spark_test_log.txt 2>&1"

  #   - name: download spark test log
  #     if: matrix.runner == 'spark' 
  #     run: |
  #       scp -o StrictHostKeyChecking=no -i ~/.ssh/local_test.pem ubuntu@node0:~/spark_test_log.txt spark_test_log.txt
  #       scp -o StrictHostKeyChecking=no -i ~/.ssh/local_test.pem ubuntu@node0:/opt/tmp_spark/PowerRuntimes.csv spark_result.csv
  #   - name: upload spark test log
  #     if: matrix.runner == 'spark' 
  #     uses: actions/upload-artifact@v3
  #     with:
  #       name: spark_test_log_${{ github.run_number }}_${{env.SCALE_FACTOR}}.txt
  #       path: spark_test_log.txt
  #   - name: upload spark result
  #     if: matrix.runner == 'spark' 
  #     uses: actions/upload-artifact@v3
  #     with:
  #       name: spark_result_${{ github.run_number }}_${{env.SCALE_FACTOR}}.csv
  #       path: spark_result.csv

  #   - name: Run chuknou first Test
  #     if: matrix.runner == 'chukonu' 
  #     run: |
  #       timeout 4h ssh -o ServerAliveInterval=60 -o ServerAliveCountMax=60 -o StrictHostKeyChecking=no -i ~/.ssh/local_test.pem ubuntu@node0 \
  #       "cd /opt/tpcds-benchmark && sudo ./run-chukonu.sh true true false false > ~/chuknou_test_log.txt 2>&1"
  #   - name: download chuknou test log
  #     if: matrix.runner == 'chukonu' 
  #     run: |
  #       scp -o StrictHostKeyChecking=no -i ~/.ssh/local_test.pem ubuntu@node0:~/chuknou_test_log.txt chuknou_test_log.txt

  #   - name: upload chuknou test log
  #     if: matrix.runner == 'chukonu' 
  #     uses: actions/upload-artifact@v3
  #     with:
  #       name: chuknou_test_log_${{ github.run_number }}_${{env.SCALE_FACTOR}}.txt
  #       path: chuknou_test_log.txt

  #   - name: Run chuknou second Test
  #     if: matrix.runner == 'chukonu' 
  #     run: |
  #       timeout 4h ssh -o ServerAliveInterval=60 -o ServerAliveCountMax=60 -o StrictHostKeyChecking=no -i ~/.ssh/local_test.pem ubuntu@node0 \
  #       "cd /opt/tpcds-benchmark && sudo ./run-chukonu.sh true true false false"
  #   # 使用 scp 将文件从 node0 拷贝到宿主
  #   - name: download tpcds result from node0
  #     if: matrix.runner == 'chukonu' 
  #     run: |
  #       scp -o StrictHostKeyChecking=no -i ~/.ssh/local_test.pem ubuntu@node0:/opt/tmp_chukonu/PowerRuntimes.csv chukonu_result.csv
  #   - name: upload chukonu result
  #     if: matrix.runner == 'chukonu' 
  #     uses: actions/upload-artifact@v3
  #     with:
  #       name: chukonu_result_${{ github.run_number }}_${{env.SCALE_FACTOR}}.csv
  #       path: chukonu_result.csv
  #   - name: Terminate EC2 instances
  #     if: always()
  #     run: |
  #       python ./.github/scripts/terminate_ec2_instances.py --runner ${{ matrix.runner }}  # 调用终止实例的 Python 脚本

  aggregate-results:
    runs-on: ubuntu-latest
    # needs: run-spark-test
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      # Step 1: 配置python
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'
      # Step 2: 安装 pip依赖
      - name: Install Boto3
        run: |
          python -m pip install --upgrade pip
          pip install pandas

      - name: Download spark result using actions/download-artifact if SPARK_RUNNER is 0
        if: env.SPARK_RUNNER == '0'
        uses: actions/download-artifact@v3
        with:
          name: spark_result_${{ github.run_number }}_${{ env.SCALE_FACTOR }}.csv

      - name: Download spark result using curl if SPARK_RUNNER is not 0
        if: env.SPARK_RUNNER != '0'
        run: |
          curl -L -H "Authorization: token ${{ secrets.ACT_TOKEN }}" \
            -o spark_result_${{ env.SPARK_RUNNER }}_${{ env.SCALE_FACTOR }}.zip \
            https://api.github.com/repos/zizdlp/aws_deploy/actions/artifacts/${{env.ARTIFACT_ID}}/zip
          
          unzip spark_result_${{ env.SPARK_RUNNER }}_${{ env.SCALE_FACTOR }}.zip -d ./spark_results
      - name: Download chukonu result
        uses: actions/download-artifact@v3
        with:
          name: chukonu_result_${{ github.run_number }}_${{env.SCALE_FACTOR}}.csv

      - name: Compare Test Results
        run: python3 ./.github/scripts/compare.py \
          --spark ./spark_result_${{ env.SPARK_RUNNER != 0 && env.SPARK_RUNNER || github.run_number }}_${{env.SCALE_FACTOR}}.csv \
          --chukonu ./chukonu_result_${{ github.run_number }}_${{env.SCALE_FACTOR}}.csv

      - name: Upload Comparison Results
        uses: actions/upload-artifact@v3
        with:
          name: tpcds_result_${{ github.run_number }}_${{env.SCALE_FACTOR}}.csv
          path: tpcds_result.csv
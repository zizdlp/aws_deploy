name: TEST_TPCDS

on:
  pull_request:
    branches:
      - main
  push:
    branches:
      - main
env:  # Define environment variables here
  NUM_INSTANCES: 4  # Set the number of instances here
  INSTANCE_TYPE: i4i.4xlarge  # Set the instance type here
  SCALE_FACTOR: 100
  SPARK_NUM_EXECUTORS: 3
  SPARK_EXECUTOR_CORES: 12
  SPARK_EXECUTOR_MEMORY: 90g
  SPARK_OFFSIZE: 0g
  SPARK_OVERHEAD: 6g
  CHUKONU_NUM_EXECUTORS: 3
  CHUKONU_EXECUTOR_CORES: 12
  CHUKONU_EXECUTOR_MEMORY: 47g
  CHUKONU_OFFSIZE: 1g
  CHUKONU_OVERHEAD: 48g
  NODEMANAGER_MEMORY_MB: 102400
  MAXIMUM_ALLOCATION_MB: 307200
  MINIMUM_ALLOCATION_MB: 2048
  MAXIMUM_ALLOCATION_VCORES: 45
jobs:
  deploy-and-test:
    runs-on: ubuntu-latest
    steps:
    # Step 1: 检出代码
    - name: Checkout code
      uses: actions/checkout@v3
    # Step 2: 安装依赖
    - name: Install deps for worker
      uses: ./.github/actions/install_deps
    # Step 3: 创建aws集群
    - name: Deploy EC2 Instances
      uses: ./.github/actions/aws_create
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        num-instances: ${{ env.NUM_INSTANCES }}
        instance-type: ${{ env.INSTANCE_TYPE }}
    # Step 4: 配置ssh & 克隆诸葛弩仓库
    - name: Clone and Compress Repo
      uses: ./.github/actions/clone_and_compress
      with:
        ssh_private_key: ${{ secrets.SSH_PRIVATE_KEY }}
        deploy_key: ${{ secrets.DEPLOY_KEY }}
    # Step 5: 测试dns 联通
    - name: Test SSH connection to master
      run: |
        ssh -i ~/.ssh/local_test.pem ubuntu@node0 'hostname'
    # Step 6: 打印环境变量
    - name: Print Environment Variables
      run: |
        echo "NUM_INSTANCES: $NUM_INSTANCES"
        echo "INSTANCE_TYPE: $INSTANCE_TYPE"

    # Step 7: 使用上面定义的env覆盖变量
    - name: Update vars with env
      run: |
        python3 ./.github/scripts/update_var.py 

    # Step 8: 读取 inventory.ini 文件内容
    - name: Read inventory.ini content
      id: read_inventory
      run: |
        cat ./ansible/inventory.ini
        echo "INVENTORY_CONTENT<<EOF" >> $GITHUB_ENV
        cat ./ansible/inventory.ini >> $GITHUB_ENV
        echo "EOF" >> $GITHUB_ENV

    # Step 9: 读取 known_hosts 文件内容并设置为环境变量
    - name: Read known_hosts content
      id: read_known_hosts
      run: |
        cat ~/.ssh/known_hosts
        echo "KNOWN_HOSTS_CONTENT<<EOF" >> $GITHUB_ENV
        cat ~/.ssh/known_hosts >> $GITHUB_ENV
        echo "EOF" >> $GITHUB_ENV

    # Step 10: 使用 Ansible 安装依赖
    - name: set ssh & nfs
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/install_deps.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}
    #Step 11: 配置nvme
    - name: use nvme
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/use_nvme.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}


    # Step 12: 使用 Ansible 部署 sbt
    - name: install sbt
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/install_sbt.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

    # Step 13: 使用 Ansible 部署 scala
    - name: install scala
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/install_scala.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}
    # Step 14: 使用 Ansible 部署 Spark 集群
    - name: install spark
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/install_spark.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

    # Step 15: 使用 Ansible 部署 hadoop 集群
    - name: install hadoop
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/install_hadoop.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

    # Step 16: 使用 Ansible 部署 mysql
    - name: install mysql
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/install_mysql.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

    # Step 17: 使用 Ansible 部署 hive
    - name: install hive
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/install_hive.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

    # Step 18: 使用 Ansible 部署 tpcds
    - name: install tpcds
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/install_tpcds.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}
    - name: install chukonu
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/install_chukonu.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

    - name: Run spark Test
      run: |
        timeout 4h  ssh -o ServerAliveInterval=60 -o ServerAliveCountMax=60 -o StrictHostKeyChecking=no -i ~/.ssh/local_test.pem ubuntu@node0 \
        "cd /opt/tpcds-benchmark && sudo ./run-spark.sh true true false false > spark_test_log.txt 2>&1"

    - name: download spark test log& result from node0
      run: |
        scp -o StrictHostKeyChecking=no -i ~/.ssh/local_test.pem ubuntu@node0:/opt/tpcds-benchmark/spark_test_log.txt spark_test_log.txt

    - name: upload spark test log
      uses: actions/upload-artifact@v3
      with:
        name: spark_test_log_${{ github.run_number }}
        path: spark_test_log.txt

    - name: Run chuknou first Test
      run: |
        timeout 4h  ssh -o ServerAliveInterval=60 -o ServerAliveCountMax=60 -o StrictHostKeyChecking=no -i ~/.ssh/local_test.pem ubuntu@node0 \
        "cd /opt/tpcds-benchmark && sudo ./run-chukonu.sh true true false false > chuknou_test_log.txt 2>&1"

    - name: download chuknou test log& result from node0
      run: |
        scp -o StrictHostKeyChecking=no -i ~/.ssh/local_test.pem ubuntu@node0:/opt/tpcds-benchmark/chuknou_test_log.txt chuknou_test_log.txt

    - name: upload chuknou test log
      uses: actions/upload-artifact@v3
      with:
        name: chuknou_test_log_${{ github.run_number }}
        path: chuknou_test_log.txt


    - name: Run chuknou second Test
      run: |
        timeout 4h ssh -o ServerAliveInterval=60 -o ServerAliveCountMax=60 -o StrictHostKeyChecking=no -i ~/.ssh/local_test.pem ubuntu@node0 \
        "cd /opt/tpcds-benchmark && sudo ./run-chukonu.sh true true false false"

    # 使用 scp 将文件从 node0 拷贝到宿主
    - name: download tpcds result from node0
      run: |
        scp -o StrictHostKeyChecking=no -i ~/.ssh/local_test.pem ubuntu@node0:/opt/tmp_spark/PowerRuntimes.csv spark_result.csv
        scp -o StrictHostKeyChecking=no -i ~/.ssh/local_test.pem ubuntu@node0:/opt/tmp_chukonu/PowerRuntimes.csv chukonu_result.csv
    # 执行 compare.py 比较脚本
    - name: compare test results
      run: |
        python3 ./.github/scripts/compare.py
    - name: Upload comparison results
      uses: actions/upload-artifact@v3
      with:
        name: tpcds_result_${{ github.run_number }}
        path: tpcds_result.csv

    # # # # Step 8: 清理 EC2 实例
    - name: Terminate EC2 instances
      if: always()
      run: |
        python ./.github/scripts/terminate_ec2_instances.py  # 调用终止实例的 Python 脚本
name: TEST_TPCDS

on:
  # Manually trigger the workflow
  workflow_dispatch:
    inputs:
      scale_factor:
        description: "Scale factor"
        required: false
        default: "100"
      spark_result:
        description: "Spark result URL"
        required: false
        default: "https://api.github.com/repos/chukonu-team/chukonu/actions/artifacts/2139084230/zip" # default use previous test case: 100g
      chukonu_result:
        description: "Chukonu result URL"
        required: false
        default: "0"
      commit_id:
        description: "Chukonu commit id"
        required: false
        default: ""
  pull_request:
    branches:
      - main
  push:
    branches:
      - main

env:
  NUM_INSTANCES: 4
  INSTANCE_TYPE:  'i4i.4xlarge'
  SCALE_FACTOR: ${{ github.event.inputs.scale_factor || 100 }}
  SPARK_NUM_EXECUTORS: 3
  SPARK_EXECUTOR_CORES: 12
  SPARK_EXECUTOR_MEMORY:  '90g'
  SPARK_OFFSIZE:   '0g' 
  SPARK_OVERHEAD:  '6g' 
  CHUKONU_NUM_EXECUTORS:  3
  CHUKONU_EXECUTOR_CORES:  12
  CHUKONU_EXECUTOR_MEMORY:  '47g'
  CHUKONU_OFFSIZE:  '1g' 
  CHUKONU_OVERHEAD:  '48g'
  NODEMANAGER_MEMORY_MB:  102400
  MAXIMUM_ALLOCATION_MB:  307200
  MINIMUM_ALLOCATION_MB:  2048
  MAXIMUM_ALLOCATION_VCORES:  45
  SPARK_RESULT: ${{ github.event.inputs.spark_result || '0' }} # default use previous test case: 100g
  CHUKONU_RESULT: ${{ github.event.inputs.chukonu_result || '0' }} # default rerun this test
  COMMIT_ID: ${{ github.event.inputs.commit_id || '' }}
  # SPARK_RESULT: '0' #  run test
  # CHUKONU_RESULT: '0' # run test
  # SPARK_RESULT: "https://api.github.com/repos/chukonu-team/chukonu/actions/artifacts/2139084230/zip" # 100g,keep only 400days,start from 2024-1104
  # CHUKONU_RESULT: "https://api.github.com/repos/chukonu-team/chukonu/actions/artifacts/2139084227/zip" # 100g,keep only 400days,start from 2024-1104

  # SPARK_RESULT: "https://api.github.com/repos/chukonu-team/chukonu/actions/artifacts/2132816123/zip" # 1000g,keep only 400days,start from 2024-1101
  # CHUKONU_RESULT: "https://api.github.com/repos/chukonu-team/chukonu/actions/artifacts/2132816119/zip" # 1000g,keep only 400days,start from 2024-1101
jobs:
  run-spark-test:
    runs-on: ubuntu-latest
    timeout-minutes: 1200
    strategy:
      matrix:
        runner: [spark,chukonuclang]
    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Install deps for worker
      uses: ./.github/actions/install_deps

    - name: Deploy EC2 Instances
      if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonuclang' && env.CHUKONU_RESULT == '0')
      uses: ./.github/actions/aws_create
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        num-instances: ${{ env.NUM_INSTANCES }}
        instance-type: ${{ env.INSTANCE_TYPE }}
        runner: ${{ matrix.runner }}
        run-number: ${{ github.run_number }}
    - name: Clone and Compress Repo
      if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonuclang' && env.CHUKONU_RESULT == '0')
      uses: ./.github/actions/clone_and_compress
      with:
        ssh_private_key: ${{ secrets.SSH_PRIVATE_KEY }}
        deploy_key: ${{ secrets.DEPLOY_KEY }}
        commit_id: ${{ env.COMMIT_ID }}

    - name: Test SSH connection to master
      if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonuclang' && env.CHUKONU_RESULT == '0')
      run: |
        ssh -i ~/.ssh/aws_test.pem ec2-user@node0 'hostname'

    - name: Print Environment Variables
      if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonuclang' && env.CHUKONU_RESULT == '0')
      run: |
        echo "NUM_INSTANCES: $NUM_INSTANCES"
        echo "INSTANCE_TYPE: $INSTANCE_TYPE"

    - name: Update vars with env
      if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonuclang' && env.CHUKONU_RESULT == '0')
      run: |
        python3 ./.github/scripts/update_var.py 

    - name: Read inventory.ini content
      if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonuclang' && env.CHUKONU_RESULT == '0')
      id: read_inventory
      run: |
        cat ./ansible/inventory.ini
        echo "INVENTORY_CONTENT<<EOF" >> $GITHUB_ENV
        cat ./ansible/inventory.ini >> $GITHUB_ENV
        echo "EOF" >> $GITHUB_ENV

    - name: Read known_hosts content
      if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonuclang' && env.CHUKONU_RESULT == '0')
      id: read_known_hosts
      run: |
        cat ~/.ssh/known_hosts
        echo "KNOWN_HOSTS_CONTENT<<EOF" >> $GITHUB_ENV
        cat ~/.ssh/known_hosts >> $GITHUB_ENV
        echo "EOF" >> $GITHUB_ENV

    - name: set ssh & nfs
      if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonuclang' && env.CHUKONU_RESULT == '0')
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/install_deps.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

    - name: use nvme
      if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonuclang' && env.CHUKONU_RESULT == '0')
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/use_nvme.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

    - name: install sbt
      if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonuclang' && env.CHUKONU_RESULT == '0')
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/install_sbt.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

    - name: install scala
      if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonuclang' && env.CHUKONU_RESULT == '0')
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/install_scala.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

    - name: install spark
      if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonuclang' && env.CHUKONU_RESULT == '0')
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/install_spark.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

    - name: install hadoop
      if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonuclang' && env.CHUKONU_RESULT == '0')
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/install_hadoop.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

    - name: install mysql 
      if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonuclang' && env.CHUKONU_RESULT == '0')
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/install_mysql.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

    - name: install hive
      if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonuclang' && env.CHUKONU_RESULT == '0')
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/install_hive.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}

    - name: install tpcds
      if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonuclang' && env.CHUKONU_RESULT == '0')
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/install_tpcds.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}
    - name: install chukonu
      if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonuclang' && env.CHUKONU_RESULT == '0')
      uses: dawidd6/action-ansible-playbook@v2
      with:
        playbook: ./ansible/install_chukonu.yml
        inventory: ${{ env.INVENTORY_CONTENT }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ env.KNOWN_HOSTS_CONTENT }}
    - name: Run gen data
      if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonuclang' && env.CHUKONU_RESULT == '0')
      run: |
        ssh -o ServerAliveInterval=60 -o ServerAliveCountMax=60 -o StrictHostKeyChecking=no -i ~/.ssh/aws_test.pem ec2-user@node0 \
        "sudo ./gen_data.sh spark ${{env.SCALE_FACTOR}}"
    - name: Run create table
      if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonuclang' && env.CHUKONU_RESULT == '0')
      run: |
        ssh -o ServerAliveInterval=60 -o ServerAliveCountMax=60 -o StrictHostKeyChecking=no -i ~/.ssh/aws_test.pem ec2-user@node0 \
        "sudo ./create_table.sh spark"

    - name: Run First Test
      if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonuclang' && env.CHUKONU_RESULT == '0')
      run: |
        ssh -o ServerAliveInterval=60 -o ServerAliveCountMax=60 -o StrictHostKeyChecking=no -i ~/.ssh/aws_test.pem ec2-user@node0 \
        "sudo ./99query.sh ${{matrix.runner}}; python3 process_data.py --runner ${{matrix.runner}}"

    - name: download first test result
      if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonuclang' && env.CHUKONU_RESULT == '0')
      run: |
        scp -o StrictHostKeyChecking=no -i ~/.ssh/aws_test.pem ec2-user@node0:~/${{matrix.runner}}_result.csv ${{matrix.runner}}_result_first.csv
    - name: upload result
      if: (matrix.runner == 'spark' && env.SPARK_RESULT == '0') || (matrix.runner == 'chukonuclang' && env.CHUKONU_RESULT == '0')
      uses: actions/upload-artifact@v3
      with:
        name: ${{matrix.runner}}_result_first.csv
        path: ${{matrix.runner}}_result_first.csv
        retention-days: 400


    - name: Run Second Test
      if: (matrix.runner == 'chukonuclang' && env.CHUKONU_RESULT == '0')
      run: |
        ssh -o ServerAliveInterval=60 -o ServerAliveCountMax=60 -o StrictHostKeyChecking=no -i ~/.ssh/aws_test.pem ec2-user@node0 \
        "sudo ./99query.sh ${{matrix.runner}}; python3 process_data.py --runner ${{matrix.runner}}"


    - name: download second test result
      if:  (matrix.runner == 'chukonuclang' && env.CHUKONU_RESULT == '0')
      run: |
        scp -o StrictHostKeyChecking=no -i ~/.ssh/aws_test.pem ec2-user@node0:~/${{matrix.runner}}_result.csv ${{matrix.runner}}_result_second.csv
    - name: upload result
      if:  (matrix.runner == 'chukonuclang' && env.CHUKONU_RESULT == '0')
      uses: actions/upload-artifact@v3
      with:
        name: ${{matrix.runner}}_result_second.csv
        path: ${{matrix.runner}}_result_second.csv
        retention-days: 400


    # Step 1: 压缩 node0 上的 /opt/event_logs
    - name: Compress event_logs on node0
      if: matrix.runner == 'chukonuclang' && env.CHUKONU_RESULT == '0'
      run: |
        ssh -o StrictHostKeyChecking=no -i ~/.ssh/aws_test.pem ec2-user@node0 "sudo tar -czf /opt/event_logs.tar.gz -C /opt event_logs"
        scp -o StrictHostKeyChecking=no -i ~/.ssh/aws_test.pem ec2-user@node0:/opt/event_logs.tar.gz event_logs.tar.gz
    - name: Upload event_logs
      if: matrix.runner == 'chukonuclang' && env.CHUKONU_RESULT == '0'
      uses: actions/upload-artifact@v3
      with:
        name: event_logs.tar.gz
        path: event_logs.tar.gz
        retention-days: 400
    - name: Compress chukonu_cache on node0
      if: matrix.runner == 'chukonuclang' && env.CHUKONU_RESULT == '0'
      run: |
        ssh -o StrictHostKeyChecking=no -i ~/.ssh/aws_test.pem ec2-user@node0 "sudo tar -czf /opt/chukonu_cache.tar.gz -C /opt chukonu_cache"
        scp -o StrictHostKeyChecking=no -i ~/.ssh/aws_test.pem ec2-user@node0:/opt/chukonu_cache.tar.gz chukonu_cache.tar.gz
    - name: Upload chukonu_cache
      if: matrix.runner == 'chukonuclang' && env.CHUKONU_RESULT == '0'
      uses: actions/upload-artifact@v3
      with:
        name: chukonu_cache.tar.gz
        path: chukonu_cache.tar.gz
        retention-days: 400

    - name: Terminate EC2 instances
      if: always()
      run: |
        python ./.github/scripts/terminate_ec2_instances.py --runner ${{ matrix.runner }} \
         --spark ${{ env.SPARK_RESULT }} \
         --chukonu ${{ env.CHUKONU_RESULT }} \
         --run-number ${{ github.run_number }}
  aggregate-results:
    runs-on: ubuntu-latest
    needs: run-spark-test
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      # Step 1: 配置python
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'
      # Step 2: 安装 pip依赖
      - name: Install Boto3
        run: |
          python -m pip install --upgrade pip
          pip install pandas

      - name: Download spark result using actions/download-artifact if SPARK_RESULT is "0"
        if: env.SPARK_RESULT == '0'
        uses: actions/download-artifact@v3
        with:
          name: spark_result_first.csv

      - name: Download spark result using curl if SPARK_RESULT is not "0"
        if: env.SPARK_RESULT != '0'
        run: |
          curl -L -H "Authorization: token ${{ secrets.ACT_TOKEN }}" \
            -o spark_result_first.zip \
            ${{env.SPARK_RESULT}}
          unzip spark_result_first.zip

      - name: Download CHUKONU_RESULT using actions/download-artifact if CHUKONU_RESULT is "0"
        if: env.CHUKONU_RESULT == '0'
        uses: actions/download-artifact@v3
        with:
          name: chukonuclang_result_second.csv

      - name: Download CHUKONU_RESULT result using curl if CHUKONU_RESULT is not "0"
        if: env.CHUKONU_RESULT != '0'
        run: |
          curl -L -H "Authorization: token ${{ secrets.ACT_TOKEN }}" \
            -o chukonuclang_result_second.zip \
            ${{env.CHUKONU_RESULT}}
          unzip chukonuclang_result_second.zip

      - name: Compare Test Results
        run: python3 ./.github/scripts/compare.py --spark ./spark_result_first.csv --chukonu ./chukonuclang_result_second.csv

      - name: Upload Comparison Results
        uses: actions/upload-artifact@v3
        with:
          name: tpcds_result_${{env.SCALE_FACTOR}}.csv
          path: tpcds_result.csv
          retention-days: 400